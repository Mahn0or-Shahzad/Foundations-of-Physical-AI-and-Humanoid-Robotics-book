---
id: rclpy-urdf-practical
title: Module 1B - Python Development and URDF Modeling
sidebar_position: 3
description: Hands-on ROS 2 development with rclpy, creating packages, and modeling humanoid robots in URDF.
keywords: [rclpy, URDF, ROS 2 Python, humanoid modeling, launch files, parameters]
---

# Module 1B: ROS 2 Python Development and Humanoid URDF Modeling

## Learning Objectives

By the end of this chapter, you will be able to:

1. Create ROS 2 Python packages using `rclpy` with proper package structure
2. Write publishers and subscribers for sensor data simulation
3. Design launch files to orchestrate multiple nodes
4. Configure parameters for runtime customization
5. Model a humanoid robot using URDF (Unified Robot Description Format)
6. Define joints, links, and sensors in URDF for simulation
7. Publish simulated sensor data (IMU, LiDAR, camera) from a humanoid model
8. Visualize and validate URDF models in RViz and Gazebo

---

## Part 1: ROS 2 Python Development with rclpy

### Setting Up Your ROS 2 Workspace

A ROS 2 workspace organizes your packages, builds, and dependencies. Follow these steps to create a workspace for humanoid robotics development:

```bash
# Create workspace directory
mkdir -p ~/humanoid_ws/src
cd ~/humanoid_ws

# Initialize workspace
colcon build  # Creates build/, install/, log/ directories

# Source the workspace
source install/setup.bash
```

**Workspace Structure**:
```
humanoid_ws/
├── src/           # Source code (your packages)
├── build/         # Build artifacts (generated by colcon)
├── install/       # Installed packages (setup scripts)
└── log/           # Build and runtime logs
```

### Creating Your First ROS 2 Python Package

Let's create a package for humanoid sensor simulation:

```bash
cd ~/humanoid_ws/src

# Create package with dependencies
ros2 pkg create --build-type ament_python humanoid_sensors \
    --dependencies rclpy std_msgs sensor_msgs geometry_msgs

cd humanoid_sensors
```

**Generated Package Structure**:
```
humanoid_sensors/
├── package.xml          # Package metadata and dependencies
├── setup.py             # Python package configuration
├── setup.cfg            # Entry points configuration
├── humanoid_sensors/    # Python module directory
│   └── __init__.py
└── test/                # Unit tests
```

**Key Files Explained**:

**`package.xml`** - Declares dependencies and metadata:
```xml
<?xml version="1.0"?>
<package format="3">
  <name>humanoid_sensors</name>
  <version>0.1.0</version>
  <description>Sensor simulation for humanoid robots</description>
  <maintainer email="you@example.com">Your Name</maintainer>
  <license>Apache-2.0</license>

  <depend>rclpy</depend>
  <depend>std_msgs</depend>
  <depend>sensor_msgs</depend>
  <depend>geometry_msgs</depend>

  <test_depend>pytest</test_depend>
</package>
```

**`setup.py`** - Configures Python package and entry points:
```python
from setuptools import setup

package_name = 'humanoid_sensors'

setup(
    name=package_name,
    version='0.1.0',
    packages=[package_name],
    install_requires=['setuptools'],
    zip_safe=True,
    maintainer='Your Name',
    maintainer_email='you@example.com',
    description='Sensor simulation for humanoid robots',
    license='Apache-2.0',
    tests_require=['pytest'],
    entry_points={
        'console_scripts': [
            'imu_publisher = humanoid_sensors.imu_publisher:main',
            'camera_publisher = humanoid_sensors.camera_publisher:main',
        ],
    },
)
```

### Writing a Publisher: IMU Sensor Simulation

Create `humanoid_sensors/imu_publisher.py`:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
import math

class ImuPublisher(Node):
    """Simulates IMU data for a humanoid robot torso."""

    def __init__(self):
        super().__init__('imu_publisher')

        # Create publisher for IMU data
        self.publisher_ = self.create_publisher(Imu, '/imu/data', 10)

        # Declare parameters
        self.declare_parameter('publish_rate', 100.0)  # Hz

        # Get parameter value
        publish_rate = self.get_parameter('publish_rate').value
        timer_period = 1.0 / publish_rate  # Convert Hz to seconds

        # Create timer to publish at specified rate
        self.timer = self.create_timer(timer_period, self.publish_imu_data)

        self.get_logger().info(f'IMU Publisher initialized at {publish_rate} Hz')

    def publish_imu_data(self):
        """Publish simulated IMU data."""
        msg = Imu()

        # Header with timestamp and frame
        msg.header.stamp = self.get_clock().now().to_msg()
        msg.header.frame_id = 'imu_link'

        # Simulated orientation (standing upright)
        msg.orientation.x = 0.0
        msg.orientation.y = 0.0
        msg.orientation.z = 0.0
        msg.orientation.w = 1.0  # Quaternion for no rotation

        # Simulated angular velocity (rad/s)
        msg.angular_velocity.x = 0.01 * math.sin(self.get_clock().now().nanoseconds * 1e-9)
        msg.angular_velocity.y = 0.0
        msg.angular_velocity.z = 0.0

        # Simulated linear acceleration (m/s²)
        msg.linear_acceleration.x = 0.0
        msg.linear_acceleration.y = 0.0
        msg.linear_acceleration.z = 9.81  # Gravity

        self.publisher_.publish(msg)

def main(args=None):
    rclpy.init(args=args)
    imu_publisher = ImuPublisher()
    rclpy.spin(imu_publisher)  # Keep node running
    imu_publisher.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**Key Concepts**:

1. **Node Inheritance**: `class ImuPublisher(Node)` - All ROS 2 nodes inherit from `rclpy.node.Node`
2. **Publishers**: `self.create_publisher(Imu, '/imu/data', 10)` - Creates publisher for `sensor_msgs/Imu` messages on `/imu/data` topic with queue size 10
3. **Parameters**: `self.declare_parameter('publish_rate', 100.0)` - Runtime-configurable values
4. **Timers**: `self.create_timer(timer_period, callback)` - Periodic execution at specified rate
5. **Logging**: `self.get_logger().info(...)` - Integrated logging system
6. **Message Structure**: `msg.header.stamp`, `msg.orientation`, etc. - Standardized message fields
7. **Spin**: `rclpy.spin(node)` - Event loop processing callbacks until interrupted

### Building and Running the Node

```bash
# Build the package
cd ~/humanoid_ws
colcon build --packages-select humanoid_sensors

# Source the workspace
source install/setup.bash

# Run the IMU publisher
ros2 run humanoid_sensors imu_publisher

# Expected output:
# [INFO] [imu_publisher]: IMU Publisher initialized at 100.0 Hz

# In another terminal, verify the topic:
ros2 topic echo /imu/data

# Expected: Stream of Imu messages at 100 Hz
```

### Writing a Subscriber: IMU Data Consumer

Create `humanoid_sensors/imu_subscriber.py`:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu

class ImuSubscriber(Node):
    """Subscribes to IMU data and logs orientation."""

    def __init__(self):
        super().__init__('imu_subscriber')

        # Create subscription
        self.subscription = self.create_subscription(
            Imu,
            '/imu/data',
            self.imu_callback,
            10  # QoS queue size
        )

        self.get_logger().info('IMU Subscriber initialized')

    def imu_callback(self, msg):
        """Process incoming IMU messages."""
        # Extract orientation (quaternion)
        qx, qy, qz, qw = msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w

        # Extract angular velocity
        wx = msg.angular_velocity.x

        # Log periodically (every 100 messages to avoid spam)
        if not hasattr(self, 'msg_count'):
            self.msg_count = 0

        self.msg_count += 1
        if self.msg_count % 100 == 0:
            self.get_logger().info(f'Orientation: qw={qw:.3f}, Angular velocity x={wx:.4f} rad/s')

def main(args=None):
    rclpy.init(args=args)
    imu_subscriber = ImuSubscriber()
    rclpy.spin(imu_subscriber)
    imu_subscriber.destroy_node()
    rclpy.shutdown()
```

**Callback Pattern**: `self.create_subscription(..., callback, ...)` - The callback function (`imu_callback`) is invoked automatically whenever a message arrives. This **event-driven** model is fundamental to ROS 2.

### Launch Files: Orchestrating Multiple Nodes

Manually starting nodes one-by-one is tedious. **Launch files** automate the process:

Create `launch/sensors_launch.py`:

```python
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        # Launch IMU publisher
        Node(
            package='humanoid_sensors',
            executable='imu_publisher',
            name='imu_pub',
            parameters=[{'publish_rate': 100.0}],
            output='screen'  # Print logs to terminal
        ),

        # Launch camera publisher
        Node(
            package='humanoid_sensors',
            executable='camera_publisher',
            name='camera_pub',
            parameters=[{'publish_rate': 30.0, 'image_width': 640, 'image_height': 480}],
            output='screen'
        ),

        # Launch IMU subscriber
        Node(
            package='humanoid_sensors',
            executable='imu_subscriber',
            name='imu_sub',
            output='screen'
        ),
    ])
```

**Launch the system**:
```bash
ros2 launch humanoid_sensors sensors_launch.py
```

All three nodes start simultaneously with configured parameters. This is how complex humanoid systems with dozens of nodes are orchestrated.

### Parameters: Runtime Configuration

Parameters allow runtime customization without code changes:

```python
# Declare parameter with default
self.declare_parameter('publish_rate', 100.0)

# Get parameter value
rate = self.get_parameter('publish_rate').value

# Use in code
timer_period = 1.0 / rate
```

**Set parameters via command line**:
```bash
ros2 run humanoid_sensors imu_publisher --ros-args -p publish_rate:=50.0
```

**Set parameters via launch file** (shown above in `Node(..., parameters=[...])`).

**Set parameters via YAML file**:

Create `config/sensors_params.yaml`:
```yaml
imu_publisher:
  ros__parameters:
    publish_rate: 100.0
    frame_id: 'imu_link'

camera_publisher:
  ros__parameters:
    publish_rate: 30.0
    image_width: 640
    image_height: 480
```

Load in launch file:
```python
Node(
    package='humanoid_sensors',
    executable='imu_publisher',
    parameters=['/path/to/sensors_params.yaml']
)
```

---

## Part 2: URDF Modeling for Humanoid Robots

### What is URDF?

The **Unified Robot Description Format (URDF)** is an XML specification for describing robot kinematics, dynamics, sensors, and visual appearance (ROS Documentation, 2023). URDF files serve as the single source of truth for a robot's physical structure, used by:

- **RViz**: Visualization tool for displaying robot models
- **Gazebo**: Physics simulator for testing robot behavior
- **MoveIt**: Motion planning library requiring kinematic structure
- **Robot State Publisher**: Computes and publishes TF transforms between robot links

### URDF Structure: Links and Joints

A URDF model consists of:
- **Links**: Rigid bodies (torso, thigh, forearm, etc.)
- **Joints**: Connections between links (revolute, prismatic, fixed)

**Kinematic Tree**: URDF defines a tree structure with one root link (typically the torso) and child links connected via joints.

### Minimal URDF Example: Single Link

```xml
<?xml version="1.0"?>
<robot name="simple_humanoid">

  <!-- Base link (torso) -->
  <link name="base_link">

    <!-- Inertial properties (mass, center of mass, inertia tensor) -->
    <inertial>
      <mass value="15.0"/>  <!-- kg -->
      <origin xyz="0 0 0.3" rpy="0 0 0"/>  <!-- Center of mass offset -->
      <inertia ixx="0.5" ixy="0.0" ixz="0.0"
               iyy="0.5" iyz="0.0"
               izz="0.3"/>  <!-- Inertia tensor (kg·m²) -->
    </inertial>

    <!-- Visual representation (what you see in RViz) -->
    <visual>
      <geometry>
        <box size="0.3 0.2 0.5"/>  <!-- 30cm × 20cm × 50cm torso -->
      </geometry>
      <material name="gray">
        <color rgba="0.5 0.5 0.5 1.0"/>  <!-- RGB + alpha -->
      </material>
    </visual>

    <!-- Collision geometry (for physics simulation) -->
    <collision>
      <geometry>
        <box size="0.3 0.2 0.5"/>  <!-- Same as visual for simple models -->
      </geometry>
    </collision>

  </link>

</robot>
```

**Key Elements**:

1. **`<inertial>`**: Physical properties required for dynamics simulation (gravity, collisions, forces)
2. **`<visual>`**: Geometry for rendering (can be complex meshes for realism)
3. **`<collision>`**: Simplified geometry for collision detection (use primitives for performance)
4. **Origin**: `xyz` (position offset) and `rpy` (roll-pitch-yaw orientation in radians)

### Adding Joints: Connecting Links

Let's add a head connected to the torso via a revolute (rotational) joint:

```xml
<!-- Head link -->
<link name="head_link">
  <inertial>
    <mass value="2.0"/>
    <origin xyz="0 0 0.1" rpy="0 0 0"/>
    <inertia ixx="0.05" ixy="0" ixz="0" iyy="0.05" iyz="0" izz="0.03"/>
  </inertial>
  <visual>
    <geometry>
      <sphere radius="0.12"/>  <!-- 12cm radius spherical head -->
    </geometry>
    <material name="white">
      <color rgba="0.9 0.9 0.9 1.0"/>
    </material>
  </visual>
  <collision>
    <geometry>
      <sphere radius="0.12"/>
    </geometry>
  </collision>
</link>

<!-- Neck joint (yaw rotation) -->
<joint name="neck_yaw" type="revolute">
  <parent link="base_link"/>    <!-- Joint connects torso... -->
  <child link="head_link"/>      <!-- ...to head -->
  <origin xyz="0 0 0.25" rpy="0 0 0"/>  <!-- Joint location on torso (25cm up) -->
  <axis xyz="0 0 1"/>            <!-- Rotation axis (Z-axis = yaw) -->
  <limit effort="20.0" lower="-1.57" upper="1.57" velocity="1.0"/>
  <!-- Effort: max torque (N·m), Lower/Upper: joint limits (rad), Velocity: max speed (rad/s) -->
</joint>
```

**Joint Types**:
- **`revolute`**: Rotational joint with limits (e.g., elbow, knee)
- **`continuous`**: Rotational joint without limits (e.g., wheel)
- **`prismatic`**: Linear sliding joint (e.g., telescoping arm)
- **`fixed`**: Rigid connection (e.g., camera mounted to head)

**Joint Parameters**:
- **`<parent>` / `<child>`**: Defines kinematic tree structure
- **`<origin>`**: Joint location and orientation relative to parent link
- **`<axis>`**: Rotation or translation direction (unit vector)
- **`<limit>`**: Mechanical constraints (critical for simulation and motion planning)

### Complete Humanoid URDF Structure (Simplified)

A realistic educational humanoid has **15 degrees of freedom** (DOF):

**Kinematic Hierarchy**:
```
base_link (torso)
├── neck_yaw → head_link
│   └── camera_link (fixed joint, sensor mount)
├── left_shoulder_pitch → left_upper_arm_link
│   └── left_elbow_pitch → left_forearm_link
│       └── left_wrist_pitch → left_hand_link (gripper)
├── right_shoulder_pitch → right_upper_arm_link
│   └── right_elbow_pitch → right_forearm_link
│       └── right_wrist_pitch → right_hand_link
├── left_hip_yaw → left_thigh_link
│   └── left_knee_pitch → left_shin_link
│       └── left_ankle_pitch → left_foot_link
└── right_hip_yaw → right_thigh_link
    └── right_knee_pitch → right_shin_link
        └── right_ankle_pitch → right_foot_link
```

**Design Considerations**:

1. **Symmetry**: Left and right arms/legs mirror each other (simplifies modeling)
2. **Joint Naming**: `<body_part>_<joint_type>` (e.g., `left_shoulder_pitch`, `right_knee_pitch`)
3. **Link Naming**: `<body_part>_link` (e.g., `left_forearm_link`, `right_foot_link`)
4. **Coordinate Frames**: Follow REP 103 - X forward, Y left, Z up (right-handed)

### Adding Sensors to URDF

Sensors are attached to links via **Gazebo plugins**. For a camera on the head:

```xml
<!-- Camera link (child of head via fixed joint) -->
<link name="camera_link">
  <visual>
    <geometry>
      <box size="0.05 0.02 0.02"/>  <!-- Small camera body -->
    </geometry>
  </visual>
</link>

<joint name="camera_joint" type="fixed">
  <parent link="head_link"/>
  <child link="camera_link"/>
  <origin xyz="0.1 0 0.05" rpy="0 0 0"/>  <!-- 10cm forward, 5cm up from head center -->
</joint>

<!-- Gazebo plugin for camera simulation -->
<gazebo reference="camera_link">
  <sensor type="camera" name="head_camera">
    <update_rate>30.0</update_rate>  <!-- 30 Hz -->
    <camera>
      <horizontal_fov>1.39626</horizontal_fov>  <!-- 80 degrees -->
      <image>
        <width>640</width>
        <height>480</height>
        <format>R8G8B8</format>  <!-- RGB -->
      </image>
      <clip>
        <near>0.1</near>  <!-- Minimum distance (m) -->
        <far>100.0</far>  <!-- Maximum distance (m) -->
      </clip>
    </camera>
    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
      <ros>
        <namespace>/camera</namespace>
        <remapping>image_raw:=image_raw</remapping>
        <remapping>camera_info:=camera_info</remapping>
      </ros>
      <frame_name>camera_link</frame_name>
    </plugin>
  </sensor>
</gazebo>
```

**Similarly for LiDAR**:
```xml
<gazebo reference="lidar_link">
  <sensor type="ray" name="lidar">
    <update_rate>10</update_rate>
    <ray>
      <scan>
        <horizontal>
          <samples>360</samples>
          <resolution>1</resolution>
          <min_angle>-3.14159</min_angle>  <!-- -180° -->
          <max_angle>3.14159</max_angle>   <!-- +180° -->
        </horizontal>
      </scan>
      <range>
        <min>0.1</min>
        <max>30.0</max>
      </range>
    </ray>
    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">
      <ros>
        <remapping>~/out:=/scan</remapping>
      </ros>
      <output_type>sensor_msgs/LaserScan</output_type>
      <frame_name>lidar_link</frame_name>
    </plugin>
  </sensor>
</gazebo>
```

**And for IMU**:
```xml
<gazebo reference="imu_link">
  <sensor type="imu" name="torso_imu">
    <update_rate>100</update_rate>
    <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">
      <ros>
        <remapping>~/out:=/imu/data</remapping>
      </ros>
      <frame_name>imu_link</frame_name>
    </plugin>
  </sensor>
</gazebo>
```

**Sensor Placement Strategy**:
- **Camera**: Head, facing forward (for object detection, navigation)
- **LiDAR**: Torso or head, 360° coverage (for obstacle detection, SLAM)
- **IMU**: Torso center of mass (for orientation, balance control)
- **Force-Torque**: Feet (for ground contact, ZMP computation) - Optional

---

## Validating and Visualizing URDF

### Check URDF Syntax

```bash
# Install URDF tools
sudo apt install liburdfdom-tools

# Validate URDF
check_urdf simple_humanoid.urdf

# Expected output:
# robot name is: simple_humanoid
# ---------- Successfully parsed urdf file ----------
```

Common errors caught: missing parent links, circular dependencies, invalid joint types.

### Visualize in RViz

```bash
# Install robot state publisher
sudo apt install ros-humble-robot-state-publisher ros-humble-joint-state-publisher-gui

# Launch visualization
ros2 launch urdf_tutorial display.launch.py model:=simple_humanoid.urdf

# Opens RViz with:
# - Robot model rendered
# - Joint sliders to move joints
# - TF frames displayed
```

**RViz allows interactive exploration**: Move joint sliders to see how the humanoid articulates. Verify that joint limits prevent unrealistic poses (knee bending backward, etc.).

### Load in Gazebo Simulation

```bash
# Launch Gazebo
ros2 launch gazebo_ros gazebo.launch.py

# Spawn humanoid robot
ros2 run gazebo_ros spawn_entity.py -entity simple_humanoid -file simple_humanoid.urdf

# Expected: Humanoid appears in Gazebo, subject to gravity and physics
```

**Physics Validation**: The robot should stand upright if balanced, or fall if unstable. Adjust inertial properties and joint positions until the model behaves realistically.

---

## Publishing Sensor Data from Simulated Humanoid

Once the URDF is loaded in Gazebo with sensor plugins, data flows automatically:

```bash
# List active topics
ros2 topic list

# Expected output:
# /camera/image_raw
# /camera/camera_info
# /scan
# /imu/data
# /joint_states

# Inspect camera images
ros2 topic hz /camera/image_raw
# average rate: 30.000 Hz

# View IMU data
ros2 topic echo /imu/data --once

# Expected: Imu message with orientation, angular_velocity, linear_acceleration
```

### Subscribing to Sensor Data in Python

```python
class SensorFusion(Node):
    """Fuses IMU and LiDAR data for state estimation."""

    def __init__(self):
        super().__init__('sensor_fusion')

        # Subscribe to multiple sensors
        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)
        self.scan_sub = self.create_subscription(LaserScan, '/scan', self.scan_callback, 10)

        # Store latest data
        self.latest_imu = None
        self.latest_scan = None

        # Publish fused state
        self.state_pub = self.create_publisher(Odometry, '/fused_state', 10)

        # Timer for fusion at 50 Hz
        self.create_timer(0.02, self.publish_fused_state)

    def imu_callback(self, msg):
        self.latest_imu = msg

    def scan_callback(self, msg):
        self.latest_scan = msg

    def publish_fused_state(self):
        if self.latest_imu is None or self.latest_scan is None:
            return  # Wait for all sensors

        # Fuse data (simplified example)
        fused_msg = Odometry()
        fused_msg.header.stamp = self.get_clock().now().to_msg()
        fused_msg.pose.pose.orientation = self.latest_imu.orientation
        # ... (additional fusion logic)

        self.state_pub.publish(fused_msg)
```

**Multi-Sensor Fusion Pattern**: Subscribe to multiple topics, store latest messages, fuse in a timer callback. This pattern is ubiquitous in robotics for combining IMU (orientation), LiDAR (environment), cameras (objects), and joint states (robot configuration).

---

## Practical Exercise: Build a Humanoid Sensor System

### Exercise 1: Create IMU and Camera Publishers

**Objective**: Simulate a humanoid's sensor suite publishing to ROS 2 topics.

**Tasks**:
1. Create a package `humanoid_sensors`
2. Implement `imu_publisher.py` (publish sensor_msgs/Imu at 100 Hz)
3. Implement `camera_publisher.py` (publish sensor_msgs/Image at 30 Hz with random noise)
4. Create launch file to start both publishers
5. Use `ros2 topic hz` and `ros2 topic echo` to verify data

**Success Criteria**:
- `/imu/data` publishes at ~100 Hz
- `/camera/image_raw` publishes at ~30 Hz
- Both messages have correct headers (timestamp, frame_id)

### Exercise 2: Build a Simple URDF Humanoid

**Objective**: Create a 3-link humanoid (torso, left leg, right leg) in URDF.

**Tasks**:
1. Define `base_link` (torso) with realistic inertial properties
2. Add `left_hip_pitch` revolute joint connecting torso to left thigh
3. Add `left_thigh_link` with appropriate mass and dimensions
4. Mirror for right leg
5. Validate URDF with `check_urdf`
6. Visualize in RViz with joint sliders

**Success Criteria**:
- URDF passes validation (no errors)
- Model displays in RViz with movable hip joints
- Legs do not interpenetrate when moved (collision geometries correct)

### Exercise 3: Integrate Sensors into URDF

**Objective**: Add IMU and camera sensors to your humanoid URDF.

**Tasks**:
1. Add `camera_link` as child of `head_link` via fixed joint
2. Add Gazebo camera plugin with ROS 2 remapping to `/camera/image_raw`
3. Add `imu_link` to torso with IMU Gazebo plugin
4. Launch in Gazebo and verify sensor topics publish

**Success Criteria**:
- `ros2 topic list` shows `/camera/image_raw` and `/imu/data`
- Camera images show Gazebo environment view
- IMU data reflects robot orientation (tilt the robot in Gazebo, see IMU values change)

### Exercise 4: Control Humanoid Joints via ROS 2

**Objective**: Send joint commands to move the humanoid in simulation.

**Tasks**:
1. Add joint position controller plugin to URDF (Gazebo ROS 2 control)
2. Write Python node that publishes to `/joint_commands` (sensor_msgs/JointState)
3. Command hip joints to move legs (e.g., swing left leg forward)
4. Observe motion in Gazebo

**Success Criteria**:
- Publishing to `/joint_commands` causes joints to move in Gazebo
- Joint limits are respected (robot cannot bend knee backward)
- Smooth motion (no jerky transitions)

---

## Best Practices for ROS 2 Humanoid Development

### 1. Modular Package Design

**Recommendation**: Separate packages by functionality:

- `humanoid_description`: URDF models, meshes, configuration files
- `humanoid_sensors`: Sensor drivers and simulators
- `humanoid_control`: Balance controllers, joint controllers
- `humanoid_navigation`: Nav2 integration, path planning
- `humanoid_bringup`: Launch files orchestrating the full system

**Rationale**: Independent packages are easier to test, reuse, and maintain.

### 2. Parameter Files for Configuration

**Avoid hardcoding** values in code. Use YAML parameter files:

```yaml
# config/imu_params.yaml
imu_publisher:
  ros__parameters:
    publish_rate: 100.0
    frame_id: 'imu_link'
    noise_stddev: 0.01  # Gaussian noise standard deviation
```

**Load in node**:
```python
self.declare_parameter('publish_rate', 100.0)
self.declare_parameter('frame_id', 'imu_link')
self.declare_parameter('noise_stddev', 0.01)

rate = self.get_parameter('publish_rate').value
frame = self.get_parameter('frame_id').value
```

**Benefits**: Change simulation parameters without recompiling code.

### 3. Namespace Organization

For multiple robots or subsystems, use **namespaces**:

```bash
# Launch with namespace
ros2 run humanoid_sensors imu_publisher --ros-args -r __ns:=/robot1

# Topics become:
# /robot1/imu/data
# /robot1/camera/image_raw
```

**Use case**: Testing multi-robot coordination or isolating subsystems during development.

### 4. Use Standard Message Types

Prefer standard messages (`sensor_msgs`, `geometry_msgs`, `nav_msgs`) over custom messages whenever possible:

- **Interoperability**: Tools like RViz, PlotJuggler work automatically
- **Documentation**: Well-documented schemas
- **Community Support**: Extensive examples and tutorials

Create custom messages only when standard types are insufficient (e.g., `TaskPlan` message in VLA system).

### 5. Log Appropriately

Use ROS 2 logging levels:

```python
self.get_logger().debug('Detailed debugging info')
self.get_logger().info('Normal operation messages')
self.get_logger().warn('Non-critical issues')
self.get_logger().error('Errors that prevent operation')
self.get_logger().fatal('Critical failures requiring shutdown')
```

**Production systems** run with `INFO` level; **debugging** uses `DEBUG` level. Avoid excessive logging in high-frequency callbacks (over 10 Hz) to prevent performance degradation.

---

## Debugging and Introspection Tools

ROS 2 provides powerful introspection tools for understanding running systems:

### View Active Nodes

```bash
ros2 node list
# Shows all running nodes

ros2 node info /imu_publisher
# Shows publishers, subscribers, services, actions for that node
```

### Inspect Topics

```bash
ros2 topic list
# Shows all active topics

ros2 topic info /imu/data
# Shows message type and publishers/subscribers

ros2 topic hz /imu/data
# Measures publication rate (verify 100 Hz)

ros2 topic echo /imu/data
# Displays messages in real-time
```

### Visualize Computational Graph

```bash
# Install rqt_graph
sudo apt install ros-humble-rqt-graph

# Launch graph visualization
rqt_graph
```

**rqt_graph** displays nodes as ovals and topics as arrows, enabling visual verification of your system architecture.

### Monitor Transform Tree

```bash
# Install TF tools
sudo apt install ros-humble-rqt-tf-tree

# Visualize TF tree
ros2 run rqt_tf_tree rqt_tf_tree

# Or view as text
ros2 run tf2_ros tf2_echo base_link camera_link
# Shows transform from base_link to camera_link
```

**TF tree validation** ensures your URDF's kinematic structure is correct and all sensor frames are properly defined.

---

## Summary and Next Steps

You now understand:

✅ **ROS 2 architecture**: Nodes, topics, services, actions, and DDS middleware
✅ **rclpy**: Python client library for ROS 2 node development
✅ **Publishers/Subscribers**: Event-driven communication for sensor data
✅ **Launch files**: Orchestrating multi-node systems
✅ **Parameters**: Runtime configuration without code changes
✅ **URDF**: Modeling humanoid robots with links, joints, and sensors
✅ **Gazebo plugins**: Simulating sensors (camera, LiDAR, IMU)
✅ **Validation tools**: check_urdf, RViz, Gazebo, introspection commands

**In Module 2**, we'll extend this foundation by creating **complete digital twin environments** in Gazebo and Unity, simulating realistic physics (gravity, collisions, friction), and exploring advanced sensor configurations for humanoid perception.

**Hands-On Next Steps**:
1. Complete Exercise 1-4 above
2. Experiment with different QoS policies (reliable vs best-effort)
3. Create a service for resetting joint positions
4. Add additional sensors to your URDF (depth camera, force-torque sensors)
5. Explore ROS 2 documentation for advanced topics (lifecycle nodes, component composition)

**Module 1 Validation Checkpoint**: You should now be able to create a ROS 2 workspace, write publishers/subscribers in Python, model a simple humanoid in URDF, launch it in Gazebo, and observe sensor data flowing through ROS 2 topics. If you can do this, you're ready for Module 2!

---

## References

Macenski, S., Foote, T., Gerkey, B., Lalancette, C., & Woodall, W. (2022). Robot Operating System 2: Design, architecture, and uses in the wild. *Science Robotics*, 7(66), eabm6074. https://doi.org/10.1126/scirobotics.abm6074

Object Management Group (2015). *Data Distribution Service (DDS) Version 1.4*. OMG Document formal/2015-04-10. https://www.omg.org/spec/DDS/1.4

Quigley, M., Gerkey, B., & Smart, W. D. (2015). *Programming Robots with ROS: A practical introduction to the Robot Operating System*. O'Reilly Media.

REP 103: Standard Units of Measure and Coordinate Conventions (2010). https://www.ros.org/reps/rep-0103.html

ROS 2 Documentation (2023). *URDF XML Specification*. Open Robotics. https://wiki.ros.org/urdf/XML
